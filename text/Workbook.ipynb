{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa30d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset_builder, get_dataset_config_names, load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "import numpy as np\n",
    "import pdb\n",
    "import logging\n",
    "import re\n",
    "from translate.storage.tmx import tmxfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead71c2",
   "metadata": {},
   "source": [
    "# 0. Hugging Face login\n",
    "- Necessary only for 'Gated' datasets on hugging face\n",
    "- Specific to user (if I request access for a gated dataset, you'll need to request access also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75161b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a42851",
   "metadata": {},
   "source": [
    "## 1. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee36fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_conversion_dict, normalize_pairs, list_languages, list_languagesG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cce754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read language pair data from both sources\n",
    "mt_hf_df = pd.read_csv('data/language_pairs_hf.csv')\n",
    "mt_ext_df = pd.read_csv('data/language_pairs_external.csv')\n",
    "mt = pd.concat([mt_hf_df, mt_ext_df])\n",
    "mt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f677e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique pairs before normalization: {len(mt['Language Pair'].unique())}\")\n",
    "print(f\"Unique datasets: {len(mt['Author/Dataset'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb86d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_mappings = create_conversion_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pairs(mt_df, iso_map) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes language pairs.\n",
    "    1. Strips away script/locale information.\n",
    "    2. Normalizes src/tgt direction\n",
    "    \"\"\"\n",
    "    scripts = r'_[A-Z][a-z]{3}'\n",
    "    endings = r'(-|_)[A-Z]{2,}'\n",
    "    misc = \"(-sursilv|-vallader|-tw|-valencia|_br|_tw)\"\n",
    "\n",
    "    for regex in [scripts, endings, misc]:\n",
    "        mt_df['Language Pair'] = mt_df['Language Pair'].str.replace(regex, \"\", regex=True)\n",
    "    \n",
    "    mt_df['Language Pair'] = mt_df['Language Pair'].str.replace(r'2', \"-\", regex=True)\n",
    "    \n",
    "    stragglers = mt_df['Language Pair'].str.split('-', expand=True)\n",
    "    stragglers = stragglers.iloc[:, 2:]\n",
    "    stragglers = stragglers[stragglers.notna().any(axis=1)]\n",
    "    mt_df = mt_df.drop(stragglers.index)\n",
    "\n",
    "    mt_df[['lang_1', 'lang_2']] = mt_df['Language Pair'].str.split('-', expand=True)\n",
    "    mt_df['lang_1'] = mt_df['lang_1'].apply(lambda x: iso_mappings.get(x, x))\n",
    "    mt_df['lang_2'] = mt_df['lang_2'].apply(lambda x: iso_mappings.get(x, x))    \n",
    "    mt_df['Language Pair'] = mt_df.apply(lambda row: f\"{tuple((row['lang_1'], row['lang_2']))}\", axis=1)\n",
    "    \n",
    "    missing_langs = mt_df[mt_df['lang_1'].isna() | mt_df['lang_2'].isna()]\n",
    "    mt_df = mt_df.drop(missing_langs.index)\n",
    "    \n",
    "    mt_df['Language Pair'] = mt_df.apply(lambda row: f\"{min(row['lang_1'], row['lang_2'])}-{max(row['lang_1'], row['lang_2'])}\", axis=1)\n",
    "    \n",
    "    return mt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc971ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mt = normalize_pairs(mt, iso_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdab05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique pairs: {len(norm_mt['Language Pair'].unique())}\")\n",
    "print(f\"Unique datasets: {len(norm_mt['Author/Dataset'].unique())}\") # one dataset dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc67f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aca61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_Glang = list_languages()\n",
    "supp_Glang_v2 = list_languagesG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_Glang.update(supp_Glang_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_Google(row, supported): \n",
    "    return row['lang_1'] in supported and row['lang_2'] in supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d935f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = norm_mt[~norm_mt.apply(lambda x: is_in_Google(row=x,supported=supp_Glang), axis=1)]\n",
    "almost = test.sort_values(by='# Train Set', ascending=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "almost.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddad8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.head()\n",
    "mt['total n_examples'] = mt['# Train Set'] + mt['# Development Set'] + mt['# Test Set']\n",
    "mt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = mt[mt['total n_examples'] > nice_to_have]\n",
    "group_c = mt[mt['total n_examples'] < needs_work]\n",
    "group_b = mt[(mt['total n_examples'] <= nice_to_have) & (mt['total n_examples'] >= needs_work)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(group_b) + len(group_a) + len(group_c) == len(mt) # nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50e769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa89a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[(data['# Development Set'] == 0) & (data['# Test Set'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['# Train Set'] < 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed699c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "almost = test.sort_values(by='# Train Set', ascending=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d836ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_google_translate(row):\n",
    "    lang_1, lang_2 = row[\"Language Pair\"].split(\"-\")\n",
    "    return lang_1 in iso_codes and lang_2 in iso_codes\n",
    "\n",
    "# Apply the function to filter out rows\n",
    "df_cleaned = almost[~almost.apply(is_in_google_translate, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3efb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned=df_cleaned.drop(3249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42545d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df_cleaned.head(10)['Language Pair']\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "    \"Asturian-Spanish\",\n",
    "    \"Norwegian Bokmål-Russian\",\n",
    "    \"French-Plateau Malagasy\",\n",
    "    \"Bodo-English\",\n",
    "    \"Russian-Veps\",\n",
    "    \"French-Kabyle\",\n",
    "    \"English-Bodo\",\n",
    "    \"English-Kashmiri\",\n",
    "    \"Plateau Malagasy-Russian\",\n",
    "    \"Montenegrin-English\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "    \"Asturian-XX\",\n",
    "    \"Norwegian Bokmål-XX\",\n",
    "    \"XX-Plateau Malagasy\",\n",
    "    \"Bodo-XX\",\n",
    "    \"XX-Veps\",\n",
    "    \"XX-Kabyle\",\n",
    "    \"XX-Bodo\",\n",
    "    \"XX-Kashmiri\",\n",
    "    \"Plateau Malagasy-XX\",\n",
    "    \"Montenegrin-XX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430657ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df_cleaned.head(10)['# Train Set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a31d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#d62728', '#e377c2', '#2ca02c', '#7f7f7f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ca30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(x_data, y_data, width=0.8, color=colors)  # Adjust width of bars\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Language Pairs')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Unsupported languages in Google Translate')\n",
    "plt.ticklabel_format(style='plain', axis='y')  # Ensure y-axis is not in scientific notation\n",
    "\n",
    "# Add value labels above bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:,}', \n",
    "             ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "\n",
    "# Adjust x-tick labels\n",
    "plt.xticks(rotation=23, ha='right', fontsize=10, fontstyle='italic')  # Rotate and adjust font size\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Ensure the layout is adjusted to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fe2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(x_data, y_data, color='blue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Language Pairs')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Language Pairs: Where to next?')\n",
    "plt.ticklabel_format(style='plain', axis='y')  # Ensure y-axis is not in scientific notation\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:,}', \n",
    "             ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=15, fontstyle='italic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ec3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d49819",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fff62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_a = set()\n",
    "\n",
    "for row in group_a.iterrows():\n",
    "    languages_a.add(row[1][5])\n",
    "    languages_a.add(row[1][6])\n",
    "    \n",
    "languages_b = set()\n",
    "\n",
    "for row in group_b.iterrows():\n",
    "    languages_b.add(row[1][5])\n",
    "    languages_b.add(row[1][6])\n",
    "    \n",
    "languages_c = set()\n",
    "\n",
    "for row in group_c.iterrows():\n",
    "    languages_c.add(row[1][5])\n",
    "    languages_c.add(row[1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"group a: {len(languages_a)}\\ngroup b: {len(languages_b)}\\ngroup c: {len(languages_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_set = languages_a | languages_b | languages_c\n",
    "print(f\"Unique languages: {len(mt_set)}\") # unique LANGUAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_b = languages_a | languages_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0294a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_b_c = languages_a | languages_b | languages_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a_b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8554d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "7100 - len(a_b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63617de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_datasets for group a: {len(group_a['Author/Dataset'].unique())}\")\n",
    "print(f\"n_datasets for group b: {len(group_b['Author/Dataset'].unique())}\")\n",
    "print(f\"n_datasets for group c: {len(group_c['Author/Dataset'].unique())}\")\n",
    "print(f\"n_datasets overall: {len(mt['Author/Dataset'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/mt_hf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd76418",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=test[test['Dataset Type'].str.contains('parallel', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c46127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible mt datasets\n",
    "len(test2) # APPROX. DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9755b4",
   "metadata": {},
   "source": [
    "## 2. Update ```language_pairs_external.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afe264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import update_pairs\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f942a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtex_pair = pd.read_csv('data/language_pairs_external.csv')\n",
    "mtex_pair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the helper function for ```update pairs```\n",
    "help(update_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bd782",
   "metadata": {},
   "source": [
    "### Multiway example using  [HornMT](https://github.com/asmelashteka/HornMT) dataset from GitHub\n",
    "The number of language pairs for a multiway is obtained with the permutation formula.\n",
    "- Change save to True to save your changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69beebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(n, r):\n",
    "    '''Returns the number of permutations.'''\n",
    "    return int(factorial(n) / factorial(n-r))\n",
    "\n",
    "val = permutations(6, 2)\n",
    "print(f\"There will be {val} distinct pairs for the HornMT dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for function\n",
    "data_auth = 'LesanAI/HornMT' # if external check the main contributor to the dataset\n",
    "langs = ['aa', 'am', 'en', 'om', 'so', 'ti']\n",
    "rows = [0, 0, 2030] # multiway datasets will have the same n_rows\n",
    "d_type = 'Multiway'\n",
    "save = False # change to True; param is False only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, rows, d_type, save)\n",
    "df.tail(30) # 30 distinct pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f9b80",
   "metadata": {},
   "source": [
    "### English-Centric example using  [Samantar](https://huggingface.co/datasets/ai4bharat/samanantar) dataset from Hugging Face\n",
    "There will be *n-1* number of language pairs for an English-Centric dataset. There will be 11 unique pairs for Samantar.\n",
    "- If the dataset doesn't exist in the ```mt_hf.csv``` dataset then you will manually add the dataset to ```mt_hf_external.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afba20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for function\n",
    "data_auth = 'ai4bharat/samanantar' # if external check the main contributor to the dataset\n",
    "configs = get_dataset_config_names(data_auth)\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44622fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = configs.copy()\n",
    "langs.append('en') # ensure English is in the list\n",
    "d_type = 'English-Centric'\n",
    "save = False # change to True; param is False only for demonstration purposes\n",
    "print(langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f62e9e",
   "metadata": {},
   "source": [
    "English-Centric datasets may not have the same n_rows! Therefore we'll create a dictionary for each unique language pair containing their (train, validation, test) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is easy if the config is similar to Samantar \n",
    "pairs = {}\n",
    "for config in configs:\n",
    "    rows = [0, 0, 0]\n",
    "    builder = load_dataset_builder(data_auth, config)\n",
    "    info = builder.info\n",
    "    for split in info.splits:\n",
    "        if split.startswith('train'):\n",
    "            rows[0] = info.splits[split].num_examples\n",
    "        if split.startswith('val'):\n",
    "            rows[1] = info.splits[split].num_examples\n",
    "        if split.startswith('test'):\n",
    "            rows[2] = info.splits[split].num_examples\n",
    "            \n",
    "    pairs[config] = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3065ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise you'll have to manually enter the number of rows or think of a programmatic solution.\n",
    "test = {}\n",
    "test['as'] = [141226, 0, 0]\n",
    "test['bn'] = [8604579, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There will be 11 distinct pairs for the Samanantar dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec95b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, pairs, d_type, save)\n",
    "df.tail(11) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bf495",
   "metadata": {},
   "source": [
    "### Simple parallel example using  [Filtered-Japanese-English-Parallel-Corpus](https://github.com/asmelashteka/HornMT) dataset from Hugging Face\n",
    "A simple parallel dataset contains only 2 language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb50104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auth = 'Moleys/Filtered-Japanese-English-Parallel-Corpus' # if external check the main contributor to the dataset\n",
    "langs = ['ja', 'en']\n",
    "rows = [10739509, 0, 0] \n",
    "d_type = 'Simple Parallel'\n",
    "save = False # change to True; param is False only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, rows, d_type, save)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'FBK-MT/mGeNTE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_dataset_config_names(dataset_name)\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0763f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(dataset_name, configs[1])\n",
    "# builder = load_dataset_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be92129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_auth = 'FBK-MT/gender-bias-PE' # if external check the main contributor to the dataset\n",
    "data_auth = dataset_name\n",
    "langs = ['en', 'it']\n",
    "rows = [0, 0, 1500] \n",
    "d_type = 'Simple Parallel'\n",
    "save = True # change to True; param is False only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5119c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, rows, d_type, save)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd3c5a",
   "metadata": {},
   "source": [
    "## 3. Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23886fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Belgium_justice.tmx', 'r') as fin:\n",
    "    file = tmxfile(fin, 'nl', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for node in file.unit_iter():\n",
    "    count += 1\n",
    "#     print(node.source, node.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.fullmatch(pattern, configs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for basic ISO language code pairs\n",
    "# pattern = r'^[a-z]{2,3}(-|2)[a-z]{2,3}$'\n",
    "pattern = r'[a-z]{2,3}((_|-)\\w+)?(-|2)[a-z]{2,3}((_|-)\\w+)?' # new pattern!\n",
    "\n",
    "# Example language code pairs\n",
    "codes = ['en-es', 'fr-de', 'zh-en', 'EN-es', 'eng-es_AM', 'ara_blahblah', 'iwslt14_de_en', 'amh_Ethi-arb_Arab']\n",
    "\n",
    "# Filter valid codes\n",
    "valid_codes = [code for code in codes if re.fullmatch(pattern, code)]\n",
    "\n",
    "print(valid_codes)  # Output: ['en-es', 'fr-de', 'zh-en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[a-z]{2,3}-[a-z]{2,3}(_-)?.*'\n",
    "# pattern = 'en-zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4221ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"aya_dataset\"\n",
    "re.search(pattern, string)\n",
    "# help(re.match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
