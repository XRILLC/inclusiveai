{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa30d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset_builder, get_dataset_config_names, load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "import numpy as np\n",
    "import pdb\n",
    "import logging\n",
    "import re\n",
    "from translate.storage.tmx import tmxfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc71af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62e0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 'VarunGumma/IN22-Conv-Doc-Level'\n",
    "# config = get_dataset_config_names(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1a0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead71c2",
   "metadata": {},
   "source": [
    "# 0. Hugging Face login\n",
    "- Necessary only for 'Gated' datasets on hugging face\n",
    "- Specific to user (if I request access for a gated dataset, you'll need to request access also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75161b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b7ff849fe2400aa8a6b07f7cdbc6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a42851",
   "metadata": {},
   "source": [
    "## 1. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee36fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_conversion_dict, list_languages, list_languages_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7cce754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author/Dataset</th>\n",
       "      <th>Language Pair</th>\n",
       "      <th># Train Set</th>\n",
       "      <th># Development Set</th>\n",
       "      <th># Test Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bretagne/Korpus-divyezhek-brezhoneg-galleg</td>\n",
       "      <td>br-fr</td>\n",
       "      <td>61503</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VishaliSekar/tamil_colloquial</td>\n",
       "      <td>ta-en</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nandhinivaradharajan14/tamil-english-colloquia...</td>\n",
       "      <td>en-ta</td>\n",
       "      <td>197110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Author/Dataset Language Pair  \\\n",
       "0         Bretagne/Korpus-divyezhek-brezhoneg-galleg         br-fr   \n",
       "1                      VishaliSekar/tamil_colloquial         ta-en   \n",
       "2  nandhinivaradharajan14/tamil-english-colloquia...         en-ta   \n",
       "\n",
       "   # Train Set  # Development Set  # Test Set  \n",
       "0        61503                  0           0  \n",
       "1           69                  0          30  \n",
       "2       197110                  0           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read language pair data from both sources\n",
    "mt_hf_df = pd.read_csv('data/language_pairs_hf.csv')\n",
    "mt_ext_df = pd.read_csv('data/language_pairs_external.csv')\n",
    "mt = pd.concat([mt_hf_df, mt_ext_df])\n",
    "mt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f677e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pairs before normalization: 2445\n",
      "Unique datasets: 561\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique pairs before normalization: {len(mt['Language Pair'].unique())}\")\n",
    "print(f\"Unique datasets: {len(mt['Author/Dataset'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb86d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_mappings = create_conversion_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5449fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pairs(mt_df, iso_map) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes language pairs.\n",
    "    1. Strips away script/locale information.\n",
    "    2. Normalizes src/tgt direction\n",
    "    \"\"\"\n",
    "    scripts = r'_[A-Z][a-z]{3}'\n",
    "    endings = r'(-|_)[A-Z]{2,}'\n",
    "    misc = \"(-sursilv|-vallader|-tw|-valencia|_br|_tw)\"\n",
    "\n",
    "    for regex in [scripts, endings, misc]:\n",
    "        mt_df['Language Pair'] = mt_df['Language Pair'].str.replace(regex, \"\", regex=True)\n",
    "    \n",
    "    mt_df['Language Pair'] = mt_df['Language Pair'].str.replace(r'2', \"-\", regex=True)\n",
    "    \n",
    "    stragglers = mt_df['Language Pair'].str.split('-', expand=True)\n",
    "    stragglers = stragglers.iloc[:, 2:]\n",
    "    stragglers = stragglers[stragglers.notna().any(axis=1)]\n",
    "    mt_df = mt_df.drop(stragglers.index)\n",
    "\n",
    "    mt_df[['lang_1', 'lang_2']] = mt_df['Language Pair'].str.split('-', expand=True)\n",
    "    mt_df['lang_1'] = mt_df['lang_1'].apply(lambda x: iso_mappings.get(x, x))\n",
    "    mt_df['lang_2'] = mt_df['lang_2'].apply(lambda x: iso_mappings.get(x, x))    \n",
    "    mt_df['Language Pair'] = mt_df.apply(lambda row: f\"{tuple((row['lang_1'], row['lang_2']))}\", axis=1)\n",
    "    \n",
    "    missing_langs = mt_df[mt_df['lang_1'].isna() | mt_df['lang_2'].isna()]\n",
    "    mt_df = mt_df.drop(missing_langs.index)\n",
    "    \n",
    "    mt_df['Language Pair'] = mt_df.apply(lambda row: f\"{min(row['lang_1'], row['lang_2'])}-{max(row['lang_1'], row['lang_2'])}\", axis=1)\n",
    "    \n",
    "    return mt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc971ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mt = normalize_pairs(mt, iso_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdab05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pairs after normalization: 1641\n",
      "Unique datasets after normalization: 560\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique pairs after normalization: {len(norm_mt['Language Pair'].unique())}\")\n",
    "print(f\"Unique datasets after normalization: {len(norm_mt['Author/Dataset'].unique())}\") # one dataset dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc67f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author/Dataset</th>\n",
       "      <th>Language Pair</th>\n",
       "      <th># Train Set</th>\n",
       "      <th># Development Set</th>\n",
       "      <th># Test Set</th>\n",
       "      <th>lang_1</th>\n",
       "      <th>lang_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bretagne/Korpus-divyezhek-brezhoneg-galleg</td>\n",
       "      <td>br-fr</td>\n",
       "      <td>61503</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>br</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VishaliSekar/tamil_colloquial</td>\n",
       "      <td>en-ta</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>ta</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nandhinivaradharajan14/tamil-english-colloquia...</td>\n",
       "      <td>en-ta</td>\n",
       "      <td>197110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaksani/english-to-telugu</td>\n",
       "      <td>en-te</td>\n",
       "      <td>420671</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ashuChufamo/parallel-corpus_en-am</td>\n",
       "      <td>am-en</td>\n",
       "      <td>27390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Author/Dataset Language Pair  \\\n",
       "0         Bretagne/Korpus-divyezhek-brezhoneg-galleg         br-fr   \n",
       "1                      VishaliSekar/tamil_colloquial         en-ta   \n",
       "2  nandhinivaradharajan14/tamil-english-colloquia...         en-ta   \n",
       "3                          jaksani/english-to-telugu         en-te   \n",
       "4                  ashuChufamo/parallel-corpus_en-am         am-en   \n",
       "\n",
       "   # Train Set  # Development Set  # Test Set lang_1 lang_2  \n",
       "0        61503                  0           0     br     fr  \n",
       "1           69                  0          30     ta     en  \n",
       "2       197110                  0           0     en     ta  \n",
       "3       420671                  0           0     en     te  \n",
       "4        27390                  0           0     en     am  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_mt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3b7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_langs = list_languages()\n",
    "supported_langs_gv2 = list_languages_google()\n",
    "supported_langs.update(supported_langs_gv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_Google(row, supported) -> pd.Series: \n",
    "    return row['lang_1'] in supported and row['lang_2'] in supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be6f71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential list of datasets/languages for NMT\n",
    "candidates = norm_mt.loc[~norm_mt.apply(lambda x: is_in_Google(row=x,supported=supported_langs), axis=1)]\n",
    "candidates = candidates.sort_values(by=\"# Train Set\", ascending=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f710db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author/Dataset</th>\n",
       "      <th>Language Pair</th>\n",
       "      <th># Train Set</th>\n",
       "      <th># Development Set</th>\n",
       "      <th># Test Set</th>\n",
       "      <th>lang_1</th>\n",
       "      <th>lang_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Helsinki-NLP/opus_dgt</td>\n",
       "      <td>bg-sh</td>\n",
       "      <td>1488507</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bg</td>\n",
       "      <td>sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>Helsinki-NLP/opus_dgt</td>\n",
       "      <td>mt-sh</td>\n",
       "      <td>1450424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mt</td>\n",
       "      <td>sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>projecte-aina/ES-AST_Parallel_Corpus</td>\n",
       "      <td>ast-es</td>\n",
       "      <td>704378</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>Helsinki-NLP/opus-100</td>\n",
       "      <td>en-nn</td>\n",
       "      <td>486055</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Helsinki-NLP/OPUS-100</td>\n",
       "      <td>en-nn</td>\n",
       "      <td>486055</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>d0rj/ru-mhr-parallel</td>\n",
       "      <td>mhr-ru</td>\n",
       "      <td>417103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mhr</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AigizK/mari-russian-parallel-corpora</td>\n",
       "      <td>mhr-ru</td>\n",
       "      <td>413841</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mhr</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Helsinki-NLP/multi_para_crawl</td>\n",
       "      <td>nb-ru</td>\n",
       "      <td>399050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nb</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Helsinki-NLP/OPUS-100</td>\n",
       "      <td>en-sh</td>\n",
       "      <td>267211</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>Helsinki-NLP/opus-100</td>\n",
       "      <td>en-sh</td>\n",
       "      <td>267211</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>liswei/coct-en-zhtw-dedup</td>\n",
       "      <td>zh-zht</td>\n",
       "      <td>216762</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>zht</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>aiana94/polynews-parallel</td>\n",
       "      <td>fr-plt</td>\n",
       "      <td>180069</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fr</td>\n",
       "      <td>plt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>alayaran/bodo_english_parallel</td>\n",
       "      <td>brx-en</td>\n",
       "      <td>149018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>brx</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Helsinki-NLP/opus-100</td>\n",
       "      <td>en-nb</td>\n",
       "      <td>142906</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>nb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Helsinki-NLP/OPUS-100</td>\n",
       "      <td>en-nb</td>\n",
       "      <td>142906</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>nb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>Lynxpda/back-translated-veps-russian</td>\n",
       "      <td>ru-vep</td>\n",
       "      <td>124771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "      <td>vep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Studeni/AMAZON-Products-2023-Arabic</td>\n",
       "      <td>arb-en</td>\n",
       "      <td>117243</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>arb</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>FrancophonIA/Kabyle-French-pairs</td>\n",
       "      <td>fr-kab</td>\n",
       "      <td>115270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kab</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Helsinki-NLP/opus-100</td>\n",
       "      <td>en-wa</td>\n",
       "      <td>104496</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Helsinki-NLP/OPUS-100</td>\n",
       "      <td>en-wa</td>\n",
       "      <td>104496</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>en</td>\n",
       "      <td>wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>AI4Bharat/BPCC</td>\n",
       "      <td>brx-en</td>\n",
       "      <td>99395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>brx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Helsinki-NLP/opus_dgt</td>\n",
       "      <td>ga-sh</td>\n",
       "      <td>91613</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ga</td>\n",
       "      <td>sh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>AI4Bharat/BPCC</td>\n",
       "      <td>en-ks</td>\n",
       "      <td>88381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>achrafothman/aslg_pc12</td>\n",
       "      <td>ase-en</td>\n",
       "      <td>87710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ase</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>aiana94/polynews-parallel</td>\n",
       "      <td>plt-ru</td>\n",
       "      <td>79603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>plt</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>AI4Bharat/BPCC</td>\n",
       "      <td>en-mni</td>\n",
       "      <td>78204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>mni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>slone/e-mordovia-articles-2023</td>\n",
       "      <td>myv-ru</td>\n",
       "      <td>76400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ru</td>\n",
       "      <td>myv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>slone/myv_ru_2022</td>\n",
       "      <td>myv-ru</td>\n",
       "      <td>74503</td>\n",
       "      <td>1500</td>\n",
       "      <td>1500</td>\n",
       "      <td>ru</td>\n",
       "      <td>myv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>AI4Bharat/BPCC</td>\n",
       "      <td>en-kok</td>\n",
       "      <td>72766</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>kok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Helsinki-NLP/opus_montenegrinsubs</td>\n",
       "      <td>cnr-en</td>\n",
       "      <td>65043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>cnr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Author/Dataset Language Pair  # Train Set  \\\n",
       "819                  Helsinki-NLP/opus_dgt         bg-sh      1488507   \n",
       "826                  Helsinki-NLP/opus_dgt         mt-sh      1450424   \n",
       "3224  projecte-aina/ES-AST_Parallel_Corpus        ast-es       704378   \n",
       "710                  Helsinki-NLP/opus-100         en-nn       486055   \n",
       "93                   Helsinki-NLP/OPUS-100         en-nn       486055   \n",
       "2812                  d0rj/ru-mhr-parallel        mhr-ru       417103   \n",
       "43    AigizK/mari-russian-parallel-corpora        mhr-ru       413841   \n",
       "547          Helsinki-NLP/multi_para_crawl         nb-ru       399050   \n",
       "81                   Helsinki-NLP/OPUS-100         en-sh       267211   \n",
       "722                  Helsinki-NLP/opus-100         en-sh       267211   \n",
       "3104             liswei/coct-en-zhtw-dedup        zh-zht       216762   \n",
       "2368             aiana94/polynews-parallel        fr-plt       180069   \n",
       "2471        alayaran/bodo_english_parallel        brx-en       149018   \n",
       "707                  Helsinki-NLP/opus-100         en-nb       142906   \n",
       "96                   Helsinki-NLP/OPUS-100         en-nb       142906   \n",
       "1904  Lynxpda/back-translated-veps-russian        ru-vep       124771   \n",
       "1978   Studeni/AMAZON-Products-2023-Arabic        arb-en       117243   \n",
       "91        FrancophonIA/Kabyle-French-pairs        fr-kab       115270   \n",
       "741                  Helsinki-NLP/opus-100         en-wa       104496   \n",
       "63                   Helsinki-NLP/OPUS-100         en-wa       104496   \n",
       "169                         AI4Bharat/BPCC        brx-en        99395   \n",
       "823                  Helsinki-NLP/opus_dgt         ga-sh        91613   \n",
       "174                         AI4Bharat/BPCC         en-ks        88381   \n",
       "2012                achrafothman/aslg_pc12        ase-en        87710   \n",
       "2400             aiana94/polynews-parallel        plt-ru        79603   \n",
       "178                         AI4Bharat/BPCC        en-mni        78204   \n",
       "3272        slone/e-mordovia-articles-2023        myv-ru        76400   \n",
       "3274                     slone/myv_ru_2022        myv-ru        74503   \n",
       "175                         AI4Bharat/BPCC        en-kok        72766   \n",
       "908      Helsinki-NLP/opus_montenegrinsubs        cnr-en        65043   \n",
       "\n",
       "      # Development Set  # Test Set lang_1 lang_2  \n",
       "819                   0           0     bg     sh  \n",
       "826                   0           0     mt     sh  \n",
       "3224                  0           0     es    ast  \n",
       "710                2000        2000     en     nn  \n",
       "93                 2000        2000     en     nn  \n",
       "2812                  0           0    mhr     ru  \n",
       "43                    0           0    mhr     ru  \n",
       "547                   0           0     nb     ru  \n",
       "81                 2000        2000     en     sh  \n",
       "722                2000        2000     en     sh  \n",
       "3104                  0           0    zht     zh  \n",
       "2368                  0           0     fr    plt  \n",
       "2471                  0           0    brx     en  \n",
       "707                2000        2000     en     nb  \n",
       "96                 2000        2000     en     nb  \n",
       "1904                  0           0     ru    vep  \n",
       "1978                  0           0    arb     en  \n",
       "91                    0           0    kab     fr  \n",
       "741                2000        2000     en     wa  \n",
       "63                 2000        2000     en     wa  \n",
       "169                   0           0     en    brx  \n",
       "823                   0           0     ga     sh  \n",
       "174                   0           0     en     ks  \n",
       "2012                  0           0    ase     en  \n",
       "2400                  0           0    plt     ru  \n",
       "178                   0           0     en    mni  \n",
       "3272                  0           0     ru    myv  \n",
       "3274               1500        1500     ru    myv  \n",
       "175                   0           0     en    kok  \n",
       "908                   0           0     en    cnr  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e18e8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42545d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df_cleaned.head(10)['Language Pair']\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "    \"Asturian-Spanish\",\n",
    "    \"Norwegian BokmÃ¥l-Russian\",\n",
    "    \"French-Plateau Malagasy\",\n",
    "    \"Bodo-English\",\n",
    "    \"Russian-Veps\",\n",
    "    \"French-Kabyle\",\n",
    "    \"English-Bodo\",\n",
    "    \"English-Kashmiri\",\n",
    "    \"Plateau Malagasy-Russian\",\n",
    "    \"Montenegrin-English\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [\n",
    "    \"Asturian-XX\",\n",
    "    \"Norwegian BokmÃ¥l-XX\",\n",
    "    \"XX-Plateau Malagasy\",\n",
    "    \"Bodo-XX\",\n",
    "    \"XX-Veps\",\n",
    "    \"XX-Kabyle\",\n",
    "    \"XX-Bodo\",\n",
    "    \"XX-Kashmiri\",\n",
    "    \"Plateau Malagasy-XX\",\n",
    "    \"Montenegrin-XX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430657ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df_cleaned.head(10)['# Train Set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a31d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#d62728', '#e377c2', '#2ca02c', '#7f7f7f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ca30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(x_data, y_data, width=0.8, color=colors)  # Adjust width of bars\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Language Pairs')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Unsupported languages in Google Translate')\n",
    "plt.ticklabel_format(style='plain', axis='y')  # Ensure y-axis is not in scientific notation\n",
    "\n",
    "# Add value labels above bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:,}', \n",
    "             ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "\n",
    "# Adjust x-tick labels\n",
    "plt.xticks(rotation=23, ha='right', fontsize=10, fontstyle='italic')  # Rotate and adjust font size\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Ensure the layout is adjusted to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fe2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(x_data, y_data, color='blue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Language Pairs')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Language Pairs: Where to next?')\n",
    "plt.ticklabel_format(style='plain', axis='y')  # Ensure y-axis is not in scientific notation\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:,}', \n",
    "             ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=15, fontstyle='italic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ec3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d49819",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9755b4",
   "metadata": {},
   "source": [
    "## 2. Update ```language_pairs_external.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afe264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import update_pairs\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f942a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtex_pair = pd.read_csv('data/language_pairs_external.csv')\n",
    "mtex_pair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the helper function for ```update pairs```\n",
    "help(update_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bd782",
   "metadata": {},
   "source": [
    "### Multiway example using  [HornMT](https://github.com/asmelashteka/HornMT) dataset from GitHub\n",
    "The number of language pairs for a multiway is obtained with the permutation formula.\n",
    "- Change save to True to save your changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69beebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(n, r):\n",
    "    '''Returns the number of permutations.'''\n",
    "    return int(factorial(n) / factorial(n-r))\n",
    "\n",
    "val = permutations(6, 2)\n",
    "print(f\"There will be {val} distinct pairs for the HornMT dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for function\n",
    "data_auth = 'LesanAI/HornMT' # if external check the main contributor to the dataset\n",
    "langs = ['aa', 'am', 'en', 'om', 'so', 'ti']\n",
    "rows = [0, 0, 2030] # multiway datasets will have the same n_rows\n",
    "d_type = 'Multiway'\n",
    "save = False # change to True; param is False only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, rows, d_type, save)\n",
    "df.tail(30) # 30 distinct pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f9b80",
   "metadata": {},
   "source": [
    "### English-Centric example using  [Samantar](https://huggingface.co/datasets/ai4bharat/samanantar) dataset from Hugging Face\n",
    "There will be *n-1* number of language pairs for an English-Centric dataset. There will be 11 unique pairs for Samantar.\n",
    "- If the dataset doesn't exist in the ```mt_hf.csv``` dataset then you will manually add the dataset to ```mt_hf_external.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afba20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for function\n",
    "data_auth = 'ai4bharat/samanantar' # if external check the main contributor to the dataset\n",
    "configs = get_dataset_config_names(data_auth)\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44622fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = configs.copy()\n",
    "langs.append('en') # ensure English is in the list\n",
    "d_type = 'English-Centric'\n",
    "save = False # change to True; param is False only for demonstration purposes\n",
    "print(langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f62e9e",
   "metadata": {},
   "source": [
    "English-Centric datasets may not have the same n_rows! Therefore we'll create a dictionary for each unique language pair containing their (train, validation, test) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is easy if the config is similar to Samantar \n",
    "pairs = {}\n",
    "for config in configs:\n",
    "    rows = [0, 0, 0]\n",
    "    builder = load_dataset_builder(data_auth, config)\n",
    "    info = builder.info\n",
    "    for split in info.splits:\n",
    "        if split.startswith('train'):\n",
    "            rows[0] = info.splits[split].num_examples\n",
    "        if split.startswith('val'):\n",
    "            rows[1] = info.splits[split].num_examples\n",
    "        if split.startswith('test'):\n",
    "            rows[2] = info.splits[split].num_examples\n",
    "            \n",
    "    pairs[config] = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3065ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise you'll have to manually enter the number of rows or think of a programmatic solution.\n",
    "test = {}\n",
    "test['as'] = [141226, 0, 0]\n",
    "test['bn'] = [8604579, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There will be 11 distinct pairs for the Samanantar dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec95b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, pairs, d_type, save)\n",
    "df.tail(11) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bf495",
   "metadata": {},
   "source": [
    "### Simple parallel example using  [Filtered-Japanese-English-Parallel-Corpus](https://github.com/asmelashteka/HornMT) dataset from Hugging Face\n",
    "A simple parallel dataset contains only 2 language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb50104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auth = 'Moleys/Filtered-Japanese-English-Parallel-Corpus' # if external check the main contributor to the dataset\n",
    "langs = ['ja', 'en']\n",
    "rows = [10739509, 0, 0] \n",
    "d_type = 'Simple Parallel'\n",
    "save = False # change to True; param is False only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, rows, d_type, save)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'FBK-MT/mGeNTE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_dataset_config_names(dataset_name)\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0763f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(dataset_name, configs[1])\n",
    "# builder = load_dataset_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be92129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_auth = 'FBK-MT/gender-bias-PE' # if external check the main contributor to the dataset\n",
    "data_auth = dataset_name\n",
    "langs = ['en', 'it']\n",
    "rows = [0, 0, 1500] \n",
    "d_type = 'Simple Parallel'\n",
    "save = True # change to True; param is False only for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5119c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_pairs(data_auth, langs, rows, d_type, save)\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd3c5a",
   "metadata": {},
   "source": [
    "## 3. Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23886fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Belgium_justice.tmx', 'r') as fin:\n",
    "    file = tmxfile(fin, 'nl', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635cd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for node in file.unit_iter():\n",
    "    count += 1\n",
    "#     print(node.source, node.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.fullmatch(pattern, configs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for basic ISO language code pairs\n",
    "# pattern = r'^[a-z]{2,3}(-|2)[a-z]{2,3}$'\n",
    "pattern = r'[a-z]{2,3}((_|-)\\w+)?(-|2)[a-z]{2,3}((_|-)\\w+)?' # new pattern!\n",
    "\n",
    "# Example language code pairs\n",
    "codes = ['en-es', 'fr-de', 'zh-en', 'EN-es', 'eng-es_AM', 'ara_blahblah', 'iwslt14_de_en', 'amh_Ethi-arb_Arab']\n",
    "\n",
    "# Filter valid codes\n",
    "valid_codes = [code for code in codes if re.fullmatch(pattern, code)]\n",
    "\n",
    "print(valid_codes)  # Output: ['en-es', 'fr-de', 'zh-en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[a-z]{2,3}-[a-z]{2,3}(_-)?.*'\n",
    "# pattern = 'en-zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4221ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"aya_dataset\"\n",
    "re.search(pattern, string)\n",
    "# help(re.match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
