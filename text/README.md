## GENERAL INFORMATION 

*This README.md establishes project level documentation about the data, particulary the "objective" and "approach".*

### OBJECTIVE
Successful machine translation systems often presuppose very large parallel datasets (tens or hundreds of million sentences). Few datasets actually exemplify highly resourced language pairs while most language pairs in the world have limited data, or non-existent data.

There is no comprehensive survey on the available datasets for machine translation and their metadata. The content of the files in this repository include the data available for machine translation datasets and the language pairs, respectively.

### FILE DIRECTORY 

*This section will help users navigate the folders and files that make up the data set.*

```bash
.
├── data/                   # Contains datasets for MT datasets & lang pairs
│   ├── logging/               
│   ├── mt_hf.csv               
│   ├── mt_external.csv
│   ├── language_pairs_hf.csv
│   ├── language_pairs_external.csv
│   └── CODEBOOK.md        
├── get_data.py             # Function for retrieving data 
├── references/             # Files for checking new/missing data
│   ├── refresh.xlsx
│   └── missing_datasets.txt                     
├── Workbook.ipynb               
├── utils.py                
├── README.md               
└── requirements.txt        
```

#### FILE LIST

*A complete list of all of the files/folders in your data set. The file’s name and a short description are included.*

- **data/** 
  - **logging/**, Folder for logged versions of the tagged hf datasets 
  - **mt_hf.csv**, MT datasets from hf (semi-automatic)
  - **mt_external.csv**, MT datasets from external resources (manual)
  - **language_pairs_hf.csv**, data containing language pairs from hf that automatically counts # of rows
  - **language_pairs_external.csv**, data containing language pairs that cannot be extracted automatically
- **references/**
  - **missing_datasets**, Datasets unable to be extracted from hf (e.g., gated or corrupted data)
  - **refresh.xlsx**, User-friendly file for viewing new, updated, and removed datasets from hf
- **get_data.py**
- **Workbook.ipynb**, Workbook for handling or showcasing the datasets
- **utils.py**, Helper program for making tagging tasks easier for manual tagging
- **requirements.txt**

## SOURCES AND METHODS
  
*This section is devoted to the “where” and “how” of the data.*

### DATA SOURCES
MT is 

### DATA COLLECTION METHODS 

* Other resources
*Include information on how the data was collected. You should provide technical information as well as information found in the  methods section of 
related papers. Remember this is about the data, not the research.*

### DATA PROCESSING METHODS 

*Include information on how the data was cleaned and/or processed, or if sharing raw data, state that here.* 

### SOFTWARE

*This section informs users about any specialized software used to generate/process the data or needed to use the data. Create a new entry for each program and/or add-on.*
 
Name:
Version:
System Requirements:
URL:
Developer:
Additional Notes: 

### EQUIPMENT
*If your data was generated by equipment or a tool running special software you should record it’s information here. Create a new entry for each piece of 
equipment.*

Manufacturer:
Model:
Embedded Software/Firmware Name: (if applicable)
Embedded Software/Firmware Version: (if applicable)
Additional Notes:

### LICENSING 

*This section should include a statement on how other people can access, reuse, and/or redistribute the data, i.e. licensing information. Template language for CC-BY and CC-0 licenses are included below, delete the license information that does not apply.* 

This work is licensed under the Creative Commons Attribution (CC-BY) 4.0 International License. 
For more information visit: [https://creativecommons.org/licenses/by/4.0 ](https://creativecommons.org/licenses/by/4.0)


This work is licensed under the Creative Commons Universal Public Domain Dedication (CC0 1.0) License. 
For more information visit: [https://creativecommons.org/publicdomain/zero/1.0/ ](https://creativecommons.org/publicdomain/zero/1.0/)

## Virtual Environment
The data can be generated with a virtual environment. 

```bash
$ git clone inclusiveai
$ cd inclusiveai/text/
$ python3 -m venv .env
$ source .env/bin/activate
$ pip install -r requirements.txt
```

## Get Data
The ```get_data.py``` script generates both a .csv and .xlsx file in ```/data``` for machine translation datasets from Hugging Face. The initial file is generated for comparision with the data refresh to highlight newly added, modified, and removed datasets. 

Initalize the .csv file:

```
python get_data.py initialize    
```

Refresh the data:
```
python get_data.py refresh
```

Retrieve the simple language pairs:
```
python get_data.py lang-pairs
```
